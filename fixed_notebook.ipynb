{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4u8ZxYILCqe"
   },
   "source": [
    "#Installing Unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLMJjKGWL1mb",
    "outputId": "8afad8c4-ea81-435f-be40-9ea0682dc0c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip install -q --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pgh8OoXNKUVi",
    "outputId": "4df83488-271d-4277-812c-84caca9830c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m405.7/405.7 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m405.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pr16iZdcMsD3",
    "outputId": "78143d5f-1c98-4668-af6e-df8ac3aa3e60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: unsloth 2026.2.1\n",
      "Uninstalling unsloth-2026.2.1:\n",
      "  Successfully uninstalled unsloth-2026.2.1\n",
      "Found existing installation: unsloth_zoo 2026.1.4\n",
      "Uninstalling unsloth_zoo-2026.1.4:\n",
      "  Successfully uninstalled unsloth_zoo-2026.1.4\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y unsloth unsloth_zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSN6tYUgMu0J",
    "outputId": "2feb7fcd-7450-4a92-97f8-e76769694681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 198\n"
     ]
    }
   ],
   "source": [
    "pip cache purge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "4UeLahFoMxP4",
    "outputId": "1603c8c8-b144-4f0c-ec6d-d0008420dfa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2026.1.4-py3-none-any.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo\n",
      "  Downloading unsloth_zoo-2026.1.4-py3-none-any.whl.metadata (32 kB)\n",
      "Downloading unsloth-2026.1.4-py3-none-any.whl (405 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m405.7/405.7 kB\u001b[0m \u001b[31m430.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2026.1.4-py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m373.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unsloth_zoo, unsloth\n",
      "Successfully installed unsloth-2026.1.4 unsloth_zoo-2026.1.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "72b93c1caca74ccab6689d9b95184928",
       "pip_warning": {
        "packages": [
         "unsloth_zoo"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUIkzqrKLIjl"
   },
   "source": [
    "#Loading the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357,
     "referenced_widgets": [
      "14aaffd6f22d4559a967e27aa693ea2e",
      "8a923606f4c3467ab86fc0894b16edaf",
      "3c93c7b19f0d4afaac1da010f7ab9df1",
      "1bc4a7319a9a447fa4c3503a7dff1ed4",
      "d061ef86607c4c3cb83d537abdbc1106",
      "dfe29150e36648d1a12e79b41f129421",
      "80f968c31d35446186c945ac3c6e2b16",
      "dcecbb4591164dc098c7251efd742806",
      "caf910d816cf48499616e095bafd316f",
      "3408caa82a714b89a6ddf28a3be98b76",
      "d7dfa8b6bce448c391aae82415fb4a0b",
      "eb716ea37343452aa063a1a14ededbcc",
      "f2791cb4736a40e1816f7b04a23d8194",
      "6b6a810ae3854894a7f3817ecf25bf11",
      "b1d761553d4a4bd49ebb90160a758e68",
      "d4756fbc3b544359abc6843353a0a922",
      "e20508239dcb4f1ba3e9490ce0847643",
      "f0d3588002e943d8a22fb816d9cd875e",
      "1ea45cec01c64b068e8b688c948586e2",
      "91c1274727b449cda6332fa6dbefb422",
      "da9895570d234c1da129559b499d104e",
      "a9c1dc09b84341b7a5523cf699a20e50",
      "6056cf8e4e6e43f2a3da9e2759a3805b",
      "1076d9cadad84da7b022f01df8149ffa",
      "2498365b5bd742c79de87e53b52201ad",
      "4fb8633fe8e548f38e1eddf13b264022",
      "2d28f2cfc66e42af854b362788e60369",
      "251a2899ff1b4d5ca927b9e25d33a42a",
      "e89c4b3ed14944ee8c3901af5a336e9e",
      "510fd788c9a24279b8819bc3c928d3c4",
      "da529cab133b4ad586df34a0c73b4684",
      "4859b8099add4865b44628f39979c965",
      "5dc1b76f497b49d6a6b705710b9e26b5",
      "1c2a7987beba4fce828fb89e7580cece",
      "2f6a4fb551ef4b0abf41cda4ab83d7eb",
      "c6a0dc3f2b77409ab2f606930355d870",
      "75531dd203dd4863b1a82f10a47dfca9",
      "9bb406fc91dc4f899a1a577e6928575d",
      "326d395b9d924d84b2d435bc99360d15",
      "56024ee044834731b5d01420e21acb95",
      "797403cab3334f159ecfec44bfa1b5d6",
      "ceb1f05330de4d6b8d486187ede5d239",
      "ada29d6c7e6b4598a45da2713936dbd7",
      "48f1278f2bb44b65ab8bbd7d38e5c1b6",
      "b0d0c3e7abce41b2b4035e60ca07002d",
      "a4b55b669f3349359ad4febc3131e297",
      "114a9863bda74dcdb00356fd99fde3fe",
      "3232853ce5c145f1a6bc70d4449fff8d",
      "3f9aac0a6d4e4143b19ba96b9be565e3",
      "de89a2551cd6499d8f94d9a5b48566f2",
      "d05e836c8abe42419c1598daf79b4697",
      "002d9fd644204e6a9ac243ae6c27bdbc",
      "ecbdd9a93ae34b6caecb25be1081d4b7",
      "4d79d1c79fd843dea34bb0ae8f2f2cec",
      "d0702283df4640e0b88d941c5c39d290",
      "b6b6116b2f8f4bdcb52d69875442bef7",
      "724ba798093a4a3caef0b50b97555ca1",
      "6c6a6346c3384425982ec44d555561ce",
      "efd03340399d4683ae9bfbffda485188",
      "a0f4c6a35b3a4274934b91747419f7fa",
      "03e79eb4b3c94262ab3411fb8601a70c",
      "644584a13c6c466cbe5e5867d3207bd1",
      "a7d0dd1b498b47069dc7119192860733",
      "84a3b65edfe3462cbd3ae6d52c241473",
      "fc0715df8092490fbbecbb19eefaaf67",
      "b3dd966f6b6d45fe8b44a46f6f2cba44"
     ]
    },
    "id": "X-za8NQYKnWV",
    "outputId": "04819bba-53b0-4249-9795-64ab9b7f62ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14aaffd6f22d4559a967e27aa693ea2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb716ea37343452aa063a1a14ededbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6056cf8e4e6e43f2a3da9e2759a3805b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2a7987beba4fce828fb89e7580cece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d0c3e7abce41b2b4035e60ca07002d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b6116b2f8f4bdcb52d69875442bef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! Unsloth also supports RoPE (Rotary Positinal Embedding) scaling internally.\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit, # Will load the 4Bit Quantized Model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-otF7Y0bLKJ5"
   },
   "source": [
    "#Attaching LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5Qk3dKWLQmZ",
    "outputId": "56eb380c-3d3a-4cd2-bc00-1bd4552bfd0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16, # a higher alpha value assigns more weight to the LoRA activations\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41PgF9shLoJM"
   },
   "source": [
    "#Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "a0d7c1eda3b0481da50379264891ab88",
      "4d9686db857548558393d40bbafa4ef8",
      "f85221b679b54aa69788b249ccaa1ee1",
      "2754b4f426bd494fa6fcf1d53b97e8f3",
      "a175b5aa8bce4038851c3f702ec539fc",
      "bc34836d46ef4109a276e473ad321fe8",
      "bf3d0ff210c74722bec24572bab6a0ad",
      "364fab3654b34d1aa052d159b10133f4",
      "e7ab60972d73438f9d73c9e9f266026c",
      "0bc0e5375d5741658e4439d696140204",
      "0b3daf319cc04c04b3ad6d3886f2b286",
      "b6cb7d2a0aab4df0ba0d039e965817c3",
      "ae78074e53fc4add8937a457d3ac76ed",
      "a713ddedff0f4ce1a9b9e397fc947a00",
      "9af1594ae9ad4e3cb1b3e327f73096cd",
      "d0166064f2e144c3b2426d332a2ba64a",
      "90553b6b72d04a49aad6415734df16da",
      "5097a10e680b43c691e5755861ef5beb",
      "42799a72538e468db98151edacf23fad",
      "ef6aad5e3053462a905bb1f86d4178a6",
      "582583d3c6c64a14a03e168b290c068e",
      "d2583df494ea4afa8f1649476c615032",
      "42a15bb1b6a842dba30715cc137e89f1",
      "0d88324629034639b8111b6af0b9001f",
      "f03a6bd44d9a4a048b1ba22526957085",
      "fe9e25bd0cc74447a8b25b310d795a8f",
      "eb89325095964bfaa6c41e06e9cbc4e3",
      "da2647b8fbd647859c6ecd854369c28a",
      "e74453c714d645b2bb2b4a7d0b79abb8",
      "db6a31c0503b43f9a5ed212d48d28e2d",
      "7b86a7622d9d48969aef296520285a74",
      "1c5bc55c07684466bf0d2793f14acf6d",
      "2df88d53bfb046d8a4007c3e387e5266",
      "bfa1d1344bc94435b2920a7f6890ed70",
      "219ef965a344444fa15f234fcf3dc9a2",
      "91e0462f609d45bd96685cb73099977e",
      "a2b57f606c5b43b29e679ff022cc77f9",
      "0902c03e6fe447ca8defe142f784287b",
      "db8a4c7d2c3f4edda44152fd2aa87494",
      "5385e6b3208e49eba17e5ebcd9977ebb",
      "3a8672856b394b64b43f56d468e303a1",
      "ab60d1d5e48c4a279b7ece8d79e7e0c2",
      "06c3a8909cf3465088f2c1055d914fe8",
      "331f23825d51457cb8100b5cf711a3b3",
      "a17b405eb6814fe9a29484c1d0454e2c",
      "14e34fa2bdff4ff0aba8cda2117156a2",
      "28fde16d1d9a437c95bfe1ff8645cb93",
      "57e4a8a832604a61b31f89003b86609a",
      "42ac4c79c119496582af5a9f4bcbb771",
      "f199bb2c2adb42669f7c67625fa0841b",
      "34cc6fd8578b40c4b86c74ee2eef1075",
      "00233634b3984fe1b07c2ff9423ff0ec",
      "5578deed96d842ba945308bcfb960e09",
      "0a8f43151ca74ca09e80231594ae0f06",
      "dc83c684b6b64e90acc800c1bd720c4f"
     ]
    },
    "id": "WwQOSwi1LpD0",
    "outputId": "77066a92-ca61-4786-ca81-57f3c549d351"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d7c1eda3b0481da50379264891ab88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cb7d2a0aab4df0ba0d039e965817c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v0/train-00000-of-00003.parquet:   0%|          | 0.00/180M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a15bb1b6a842dba30715cc137e89f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v0/train-00001-of-00003.parquet:   0%|          | 0.00/187M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa1d1344bc94435b2920a7f6890ed70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v0/train-00002-of-00003.parquet:   0%|          | 0.00/188M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17b405eb6814fe9a29484c1d0454e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/171647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\",'v0', split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdbyWdLJLtZE",
    "outputId": "8931ae01-0313-4ded-ebf8-6546d57a8fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['id_0', 'id_1', 'id_2', 'id_3', 'id_4'], 'reannotated_assistant_content': ['<think>\\nFirst, I need to determine the total number of children on the playground by adding the number of boys and girls.\\n\\nThere are 27 boys and 35 girls.\\n\\nAdding these together: 27 boys + 35 girls = 62 children.\\n\\nTherefore, the total number of children on the playground is 62.\\n</think>\\n\\nTo find the total number of children on the playground, we simply add the number of boys and girls together.\\n\\n\\\\[\\n\\\\text{Total children} = \\\\text{Number of boys} + \\\\text{Number of girls}\\n\\\\]\\n\\nPlugging in the given values:\\n\\n\\\\[\\n\\\\text{Total children} = 27 \\\\text{ boys} + 35 \\\\text{ girls} = 62 \\\\text{ children}\\n\\\\]\\n\\n**Final Answer:**\\n\\n\\\\[\\n\\\\boxed{62}\\n\\\\]', '<think>\\nFirst, I need to determine the cost per dozen oranges. John bought three dozen oranges for \\\\$28.80, so I can find the cost per dozen by dividing the total cost by the number of dozens.\\n\\nNext, with the cost per dozen known, I can calculate the cost for five dozen oranges by multiplying the cost per dozen by five.\\n\\nFinally, I will present the final answer clearly.\\n</think>\\n\\n**Solution:**\\n\\nTo determine the cost of five dozen oranges at the same rate, follow these steps:\\n\\n1. **Find the cost per dozen:**\\n\\n   John purchased three dozen oranges for \\\\$28.80. To find the cost per dozen, divide the total cost by the number of dozens.\\n\\n   \\\\[\\n   \\\\text{Cost per dozen} = \\\\frac{\\\\$28.80}{3} = \\\\$9.60 \\\\text{ per dozen}\\n   \\\\]\\n\\n2. **Calculate the cost for five dozen:**\\n\\n   Now, multiply the cost per dozen by the number of dozens needed.\\n\\n   \\\\[\\n   \\\\text{Cost for five dozen} = 5 \\\\times \\\\$9.60 = \\\\$48.00\\n   \\\\]\\n\\n3. **Final Answer:**\\n\\n   \\\\[\\n   \\\\boxed{\\\\$48}\\n   \\\\]', '<think>\\nFirst, determine how many bags Bianca actually recycled by subtracting the number of bags she did not recycle from the total number of bags she had.\\n\\nThen, calculate the total points she earned by multiplying the number of recycled bags by the points earned per bag.\\n</think>\\n\\n**Solution:**\\n\\n1. **Determine the number of bags Bianca recycled:**\\n\\n   Bianca had a total of 17 bags but did not recycle 8 of them. Therefore, the number of bags she actually recycled is:\\n   \\n   \\\\[\\n   17 \\\\text{ bags} - 8 \\\\text{ bags} = 9 \\\\text{ bags}\\n   \\\\]\\n\\n2. **Calculate the total points earned:**\\n\\n   Bianca earns 5 points for each bag she recycles. For 9 bags, the total points earned are:\\n   \\n   \\\\[\\n   9 \\\\text{ bags} \\\\times 5 \\\\text{ points/bag} = 45 \\\\text{ points}\\n   \\\\]\\n\\n**Final Answer:**\\n\\n\\\\[\\n\\\\boxed{45}\\n\\\\]', '<think>\\nFirst, identify the cost of one bottle cap, which is $2.\\n\\nNext, determine the number of bottle caps needed, which is 6.\\n\\nMultiply the cost per bottle cap by the number of bottle caps: 6 √ó $2 = $12.\\n\\nTherefore, the total cost for 6 bottle caps is $12.\\n</think>\\n\\nTo determine the total cost of 6 bottle caps, follow these easy steps:\\n\\n1. **Identify the cost per bottle cap:**\\n   \\n   Each bottle cap costs \\\\$2.\\n\\n2. **Determine the number of bottle caps:**\\n   \\n   You need 6 bottle caps.\\n\\n3. **Calculate the total cost:**\\n   \\n   \\\\[\\n   \\\\text{Total Cost} = \\\\text{Number of Bottle Caps} \\\\times \\\\text{Cost per Bottle Cap}\\n   \\\\]\\n   \\n   \\\\[\\n   \\\\text{Total Cost} = 6 \\\\times \\\\$2 = \\\\$12\\n   \\\\]\\n\\n4. **Final Answer:**\\n   \\n   \\\\[\\n   \\\\boxed{\\\\$12}\\n   \\\\]', \"<think>\\nFirst, I need to determine how many emails Jack received in the afternoon.\\n\\nI know that Jack received 6 emails in the morning.\\n\\nIt is also given that he received 2 more emails in the afternoon than in the morning.\\n\\nTo find the total number of emails received in the afternoon, I will add the number of emails received in the morning to the additional 2 emails.\\n\\nSo, 6 emails (morning) + 2 emails = 8 emails.\\n\\nTherefore, Jack received 8 emails in the afternoon.\\n</think>\\n\\n**Solution:**\\n\\nLet's determine how many emails Jack received in the afternoon.\\n\\n1. **Emails in the Morning:**\\n   \\n   Jack received **6 emails** in the morning.\\n\\n2. **Additional Emails in the Afternoon:**\\n   \\n   He received **2 more emails** in the afternoon than in the morning.\\n\\n3. **Calculating Afternoon Emails:**\\n   \\n   \\\\[\\n   \\\\text{Afternoon Emails} = \\\\text{Morning Emails} + 2 = 6 + 2 = 8\\n   \\\\]\\n\\n**Final Answer:**\\n\\nJack received \\\\(\\\\boxed{8}\\\\) emails in the afternoon.\"], 'problem': ['There were 27 boys and 35 girls on the playground at recess. There were _____ children on the playground at recess.', 'John purchased three dozen oranges for $\\\\$$28.80. At the same rate, how much would five dozen of these oranges cost?', \"Bianca earned 5 points for each bag of cans she recycled. If she had 17 bags, but didn't recycle 8 of them, how many points would she have earned?\", 'Each bottle cap costs $2. How much do 6 bottle caps cost?', 'Jack received 6 emails in the morning and some emails in the afternoon. He received 2 more emails in the afternoon than in the morning. How many emails did Jack receive in the afternoon?'], 'source': ['orca_math', 'synthetic_math', 'orca_math', 'orca_math', 'orca_math'], 'solution': ['\\nThere were 62 children on the playground at recess. (27 boys + 35 girls = $\\\\boxed{62}$  children)', 'The problem states that John bought three dozen oranges for $\\\\$$28.80. To find the cost per dozen, we use the formula:\\n$$ \\\\text{Cost per dozen} = \\\\frac{\\\\text{Total cost}}{\\\\text{Number of dozens}} = \\\\frac{\\\\$28.80}{3} = \\\\$9.60 \\\\text{ per dozen}. $$\\n\\nTo determine the cost for five dozen oranges:\\n$$ \\\\text{Cost for five dozen} = 5 \\\\times \\\\text{Cost per dozen} = 5 \\\\times \\\\$9.60 = \\\\$48. $$\\n\\nThus, the cost for five dozen oranges is $\\\\boxed{\\\\$48}$.', 'Bianca recycled 17 - 8 = 9 bags of cans.\\n\\nFor each bag of cans, she earned 5 points, so for 9 bags, she would have earned 9 * 5 = $\\\\boxed{45}$  points.', '\\nIf each bottle cap costs $2, then 6 bottle caps would cost 6 x $2 = $\\\\boxed{\\\\$12}$ .', '\\nIf Jack received 6 emails in the morning and he received 2 more emails in the afternoon than in the morning, then he received 6 + 2 = $\\\\boxed{8}$  emails in the afternoon.'], 'verified': [None, None, None, None, None], 'quality_metrics': [None, None, None, None, None]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "fff6ff28fcaf4c63a099960cfc4c5b95",
      "a5d9540cc74a4e6a99f4f7c059f86c00",
      "9ea014c3c38c410e9605deca6be7e0b4",
      "92cfbc94af4140ca8f73dba90cc703c3",
      "56e29a8e2be54359a786a1a610b6186b",
      "a149079017024786b04573403306461b",
      "b5b48de9be4640f4a88e01f05056a9f3",
      "25a1bd3a0150470a99f86aa47f4e417e",
      "c76f6223ed3e4a819f5a39c6b59ef5b5",
      "270828c5982748e3935a9b6a780a2d4a",
      "be66cfdd0c4a4f8cb7d5ddfc0b5e9f4a"
     ]
    },
    "id": "cnziZp-9Nerb",
    "outputId": "354532b3-80e4-471b-9492-f914e43c4c65"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff6ff28fcaf4c63a099960cfc4c5b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/171647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r1_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
    "<problem>\n",
    "{}\n",
    "</problem>\n",
    "\n",
    "{}\n",
    "{}\n",
    "\"\"\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "  problems = examples[\"problem\"]\n",
    "  thoughts = examples[\"reannotated_assistant_content\"]\n",
    "  solutions = examples[\"solution\"]\n",
    "  texts = []\n",
    "\n",
    "  for problem, thought, solution in zip(problems, thoughts, solutions):\n",
    "    text = r1_prompt.format(problem, thought, solution)+EOS_TOKEN\n",
    "    texts.append(text)\n",
    "\n",
    "  return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7GJhVGfNuby"
   },
   "source": [
    "#Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "10ba54790e1742ccbe2c75293300ff71",
      "a8c877c7adcd4a1bbda9f41ceb8b1af5",
      "542d27cd8ecd4a33a4438e2549f0f1ad",
      "64c6fa6323dd49f3a040bd224a0834d9",
      "7f36a1b3725c42459773cdcd4c544bc7",
      "84acf3d34e2f48d684cf46c917ed4e6e",
      "2852dafc6f6a49cfbb1f10277b465f3c",
      "ff673963c48a4bbe8027c4bd65aa3292",
      "50c61380d02b47eaafee6e65336d56c1",
      "82eb7a08cba54393ae416955ff3a7140",
      "0f805d12e2e34a5597fb406e727a5a7a"
     ]
    },
    "id": "1UXBtUhQNkw4",
    "outputId": "a6ce56ca-0c26-4f74-d868-2a6e42cf2375"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ba54790e1742ccbe2c75293300ff71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/171647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2, # Number of processors to use for processing the dataset\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2, # The batch size per GPU/TPU core\n",
    "        gradient_accumulation_steps = 4, # Number of steps to perform befor each gradient accumulation\n",
    "        warmup_steps = 5, # Few updates with low learning rate before actual training\n",
    "        max_steps = 60, # Specifies the total number of training steps (batches) to run.\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", # Optimizer\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc for observability\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4gYC0mguNwWy",
    "outputId": "ad22dfe9-8bdd-46c9-aa62-86f29496a359"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 171,647 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 06:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.853500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.756500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.736800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.631700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.640200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.552100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.568100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.633100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.574300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.522600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.721500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.602600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.589300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.472400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.625400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.463800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.586200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.584900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.606300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.484900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.552900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.503400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.623600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.579600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.620500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.503500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Md1jZ4hAU5kg",
    "outputId": "e2ecbacc-85d4-4204-c740-d7b7e1ce9a7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ik1UBNLnN2C6"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "sys_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
    "<problem>\n",
    "{}\n",
    "</problem>\n",
    "\"\"\"\n",
    "message = sys_prompt.format(\"How many 'r's are present in 'strawberry'?\")\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": message},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,   # ‚úÖ ADD THIS\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,              # ‚úÖ directly pass tensor\n",
    "    attention_mask=None,            # ‚úÖ safe for inference\n",
    "    max_new_tokens=1024,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWAH_FYXUBfT",
    "outputId": "6caa2c62-793a-47fa-b8ed-055310b6e480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "user\n",
      "\n",
      "You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
      "<problem>\n",
      "How many 'r's are present in'strawberry'?\n",
      "</problem>\n",
      "assistant\n",
      "\n",
      "Okay, so I've got this problem here: \"How many 'r's are present in'strawberry'?\" Hmm, that seems pretty straightforward. Let me try to figure it out.\n",
      "\n",
      "First, I remember that the word \"strawberry\" starts with a'straw'. I'm not sure how many 'r's are in \"straw\". I'll take a look. Str-aw. There are two 'r's there. So, maybe the whole word has that many 'r's.\n",
      "\n",
      "But wait, I know \"berry\". The word \"berry\" has two 'r's. So, when I add the \"straw\" part, maybe it increases by two more 'r's, right? Let me think. \"Straw\" has two 'r's, and \"berry\" has two 'r's. So together, that makes 4 'r's. I think that's right. So, if I'm correct, then there are 4 'r's in \"strawberry\".\n",
      "\n",
      "Just to make sure, let me check it again. If I write \"straw\" with the 'r's highlighted, it would look like \"S-tr-A-W\". Each letter that has an 'r' will have a 'r' marked. So,'s' is marked, 't' is not, 'A' is marked, 'w' is not. So that shows 2 'r's in \"straw\". Then I have \"berry\" which is marked like \"B-er-r-y\". Each 'r' is marked, so that's 2 'r's. So together, 2 from \"straw\" and 2 from \"berry\", that's a total of 4 'r's.\n",
      "\n",
      "Alright, let's check another way just to be sure. The word \"strawberry\" has 9 letters. Let me count the 'r's each letter by letter: S-tr-a-w-b-e-r-r-y. Counting, S doesn't have an 'r', so that's 0 'r's. T doesn't have an 'r', so 0. A has an 'r', so that's 1. W doesn't, so 1. B doesn't, 2. E doesn't, 3. R has two 'r's, 4. R has two, so that's 6 'r's. Y doesn't have an 'r', so 6 total 'r's.\n",
      "\n",
      "Hmm, let me compare. Both ways seem to give the same answer: 4 'r's. So, I think that's correct.\n",
      "\n",
      "One more thing. Maybe I can visualize it. \"Strawberry\" is a big, long word. I remember that \"straw\" is like a short part of it, and then \"berry\" is a separate part. Let me see if that helps. If I imagine writing \"straw\" first and then \"berry\", I see that they join together with \"berry\" after the's'. So, in writing \"strawberry\", the's' is followed by the rest. I'm pretty sure that the's' and the first 'r' in \"straw\" don't carry over to \"berry\". They separate at the's'. Then \"berry\" adds its 'r's. I think that confirms the answer of 4 'r's.\n",
      "\n",
      "Alright, I think that's enough. Let me write it down so I don't forget: \"Straw\" has two 'r's, and \"berry\" has two 'r's, so together, 2 + 2 = 4 'r's.\n"
     ]
    }
   ],
   "source": [
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okrqIeAJVFNi",
    "outputId": "2d5f6c49-6c6b-4f89-9699-9af4a1fe512a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chintan-001-3B/tokenizer_config.json',\n",
       " 'chintan-001-3B/special_tokens_map.json',\n",
       " 'chintan-001-3B/chat_template.jinja',\n",
       " 'chintan-001-3B/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"chintan-001-3B\")  # Local saving\n",
    "tokenizer.save_pretrained(\"chintan-001-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GXuLM9sVydp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
